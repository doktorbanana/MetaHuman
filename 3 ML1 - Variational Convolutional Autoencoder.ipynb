{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae2036a-a6cd-4ffb-93d9-588ec932d76d",
   "metadata": {},
   "source": [
    "# ML1 - Autoencoder for Snippets\n",
    "Now that we have the snippets of all songs, we can use them to train ML1. We will use the architecture of a variational convolutional autoencoder. The code relies heavily on the tutorial of Valerio Velardo (see here: https://www.youtube.com/watch?v=Ey8IZQl_lKs&list=PL-wATfeyAMNpEyENTc-tVH5tfLGKtSWPp)\n",
    "We use a variational convolutional autoencoder as we hope that the latent space is more likely to be centered around the origin of a coordinate system and more evenly filled, then with a common autoencoder. That hopefully facilitates the task for ML2 - as can be seen later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5f819-82f2-41fd-b840-37bdea7e47fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## VAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2a80cb-910d-466c-ba69-fc6fc8438326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, \\\n",
    "    Conv2DTranspose, Activation, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------\n",
    "AUTOENCODER CLASS\n",
    "-------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class VAE:\n",
    "\n",
    "    def __init__(self, input_shape, conv_filters, conv_kernels, conv_strides, latent_space_dim, num_of_train_data=0):\n",
    "        self.input_shape = input_shape  # dimension of input data: frequency-bins, time-windows, amplitude\n",
    "        self.conv_filters = conv_filters  # a list with the number of filters per layer\n",
    "        self.conv_kernels = conv_kernels  # a list with the kernel size per layer\n",
    "        self.conv_strides = conv_strides  # a list with the strides per layer\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        self.reconstruction_loss_weight = 1000000\n",
    "        self._shape_before_bottleneck = None\n",
    "\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.model = None\n",
    "        self.model_input = None\n",
    "        self.num_of_train_data = num_of_train_data\n",
    "\n",
    "        self._num_conv_layers = len(conv_filters)\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "        self._build_autoencoder()\n",
    "\n",
    "    \"\"\" \n",
    "    ------------\n",
    "    Encoder Part \n",
    "    ------------\n",
    "    \"\"\"\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        encoder_input = Input(shape=self.input_shape, name=\"encoder_input\")\n",
    "        conv_layers = self._add_conv_layers(encoder_input)\n",
    "        bottleneck = self._add_bottleneck(conv_layers)\n",
    "        self._model_input = encoder_input\n",
    "        self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
    "\n",
    "    def _add_conv_layers(self, encoder_input):\n",
    "        x = encoder_input\n",
    "\n",
    "        for layer_index in range(self._num_conv_layers):\n",
    "            x = self._add_conv_layer(layer_index, x)\n",
    "        return x\n",
    "\n",
    "    def _add_conv_layer(self, layer_index, x):\n",
    "        conv_layer = Conv2D(\n",
    "            filters=self.conv_filters[layer_index],\n",
    "            kernel_size=self.conv_kernels[layer_index],\n",
    "            strides=self.conv_strides[layer_index],\n",
    "            padding=\"same\",\n",
    "            name=f\"encoder_conv_layer_{layer_index + 1}\")\n",
    "        x = conv_layer(x)  # apply the new conv to x -> Convolution with kernels give multiple 2D Arrays\n",
    "        x = ReLU(name=f\"encoder_relu_{layer_index + 1}\")(x)  # apply a ReLU activation to x\n",
    "        x = BatchNormalization(name=f\"encoder_bn_{layer_index + 1}\")(\n",
    "            x)  # apply Batch Normalization to x\n",
    "        # (less overfitting-problems, no vanishing Gradient or exploding Gradient)\n",
    "        return x\n",
    "\n",
    "    def _add_bottleneck(self, x):\n",
    "        self._shape_before_bottleneck = K.int_shape(x)[1:]  # Ignore the first dim, which is the batch size\n",
    "        x = Flatten(name=\"encoder_flatten\")(x)  # Flatten Data\n",
    "\n",
    "        # Gaussian Sampling: Sample a point in the gaussian distribution from a point in standard normal distribution\n",
    "        self.mu = Dense(self.latent_space_dim, name=\"min_vector_mu\")(x)\n",
    "        self.log_variance = Dense(self.latent_space_dim, name=\"log_variance\")(x)\n",
    "\n",
    "        \"\"\"\n",
    "        def sample_point_from_normal_dist(args):\n",
    "            mu, log_variance = args\n",
    "            epsilon = K.random_normal(shape=K.shape(self.latent_space_dim), mean=0., stddev=1.)\n",
    "            sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
    "            return sampled_point\n",
    "\n",
    "        x = Lambda(sample_point_from_normal_dist,\n",
    "                   name=\"encoder_output\")([self.mu, self.log_variance])\n",
    "        \"\"\"\n",
    "        x = LambdaLayer(self.latent_space_dim)([self.mu, self.log_variance])\n",
    "        return x\n",
    "\n",
    "    \"\"\"\n",
    "    ------------\n",
    "    Decoder Part \n",
    "    ------------\n",
    "    \"\"\"\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        decoder_input = Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
    "        dense_layer = self._add_dense_layer(decoder_input)\n",
    "        reshape_layer = Reshape(self._shape_before_bottleneck, name=\"decoder_reshape_layer\")(dense_layer)\n",
    "        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
    "        decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
    "        self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "\n",
    "    def _add_dense_layer(self, decoder_input):\n",
    "        num_neurons = np.prod(\n",
    "            self._shape_before_bottleneck)  # Product of the dimensions before the latent space\n",
    "        # -> Size of the flattened data before Latent Space\n",
    "        dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
    "        return dense_layer\n",
    "\n",
    "    def _add_conv_transpose_layers(self, x):\n",
    "        for layer_index in reversed(range(1,\n",
    "                                          self._num_conv_layers)):  # go backwards trough layers.\n",
    "            # Ignore the first layer, because we don't need\n",
    "            # ReLU or BN on it\n",
    "            x = self._add_conv_transpose_layer(layer_index, x)\n",
    "        return x\n",
    "\n",
    "    def _add_conv_transpose_layer(self, layer_index, x):\n",
    "        conv_transpose_layer = Conv2DTranspose(\n",
    "            filters=self.conv_filters[layer_index],\n",
    "            kernel_size=self.conv_kernels[layer_index],\n",
    "            strides=self.conv_strides[layer_index],\n",
    "            padding=\"same\",\n",
    "            name=f\"decoder_conv_transpose_layer_{self._num_conv_layers - layer_index}\"\n",
    "        )\n",
    "        x = conv_transpose_layer(x)\n",
    "        x = ReLU(name=f\"decoder_relu_{self._num_conv_layers - layer_index}\")(x)\n",
    "        x = BatchNormalization(name=f\"decoder_bn_{self._num_conv_layers - layer_index}\")(x)\n",
    "        return x\n",
    "\n",
    "    def _add_decoder_output(self, x):\n",
    "        conv_transpose_layer = Conv2DTranspose(\n",
    "            filters=self.input_shape[-1],  # We want to recreate the input shape\n",
    "            kernel_size=self.conv_kernels[0],\n",
    "            strides=self.conv_strides[0],\n",
    "            padding=\"same\",\n",
    "            name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
    "        )\n",
    "        x = conv_transpose_layer(x)\n",
    "        output_layer = Activation(\"sigmoid\", name=\"decoder_output_sigmoid\")(x)\n",
    "        return output_layer\n",
    "\n",
    "    \"\"\"\n",
    "    ------------\n",
    "    VAE Part \n",
    "    ------------\n",
    "    \"\"\"\n",
    "\n",
    "    def compile_model(self, learning_rate=0.0001):\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        self.model.compile(optimizer=optimizer,\n",
    "                           loss=self._calculate_combined_loss,\n",
    "                           metrics=[self.calculate_reconstruction_loss])\n",
    "\n",
    "    def _calculate_combined_loss(self, y_true, y_pred):\n",
    "        reconstruction_loss = self.calculate_reconstruction_loss(y_true, y_pred)\n",
    "        kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) - K.exp(self.log_variance), axis=1)\n",
    "        loss = (self.reconstruction_loss_weight * reconstruction_loss) + kl_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_reconstruction_loss(y_true, y_pred):\n",
    "        # Mean Squared Error\n",
    "        error = y_true - y_pred\n",
    "        reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])  # mean squared error\n",
    "        return reconstruction_loss\n",
    "\n",
    "    def train(self, x_train, x_test, batch_size, num_epochs):\n",
    "        self.num_of_train_data += x_train.shape[0]\n",
    "        hist = self.model.fit(x_train,\n",
    "                              x_train,\n",
    "                              batch_size=batch_size,\n",
    "                              epochs=num_epochs,\n",
    "                              shuffle=True,\n",
    "                              validation_data=(x_test, x_test)\n",
    "                              )\n",
    "        return hist\n",
    "\n",
    "    def _build_autoencoder(self):\n",
    "        model_input = self._model_input\n",
    "        model_output = self.decoder(self.encoder(model_input))\n",
    "        self.model = Model(model_input, model_output, name=\"Valerio\")\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    \"\"\"\n",
    "    ------------------\n",
    "    Saving and Loading\n",
    "    ------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def save(self, save_folder=\".\"):\n",
    "        self._create_folder(save_folder)\n",
    "        self._save_parameters(save_folder)\n",
    "        self._save_weights(save_folder)\n",
    "        self._save_optimizer_state(save_folder)\n",
    "        model_save_path = os.path.join(save_folder, \"model.h5\")\n",
    "        self.model.save(model_save_path)\n",
    "\n",
    "    def _create_folder(self, save_folder):\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "\n",
    "    def _save_parameters(self, save_folder):\n",
    "        parameters = [\n",
    "            self.input_shape,\n",
    "            self.conv_filters,\n",
    "            self.conv_kernels,\n",
    "            self.conv_strides,\n",
    "            self.latent_space_dim,\n",
    "            self.num_of_train_data\n",
    "        ]\n",
    "\n",
    "        save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
    "\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(parameters, f)\n",
    "\n",
    "    def _save_weights(self, save_folder):\n",
    "        save_path = os.path.join(save_folder, \"weights.h5\")\n",
    "        self.model.save_weights(save_path)\n",
    "\n",
    "    def _save_optimizer_state(self, save_folder):\n",
    "        # Save optimizer weights.\n",
    "        symbolic_weights = getattr(self.model.optimizer, 'weights')\n",
    "        weight_values = K.batch_get_value(symbolic_weights)\n",
    "        optimizer_path = os.path.join(save_folder, \"optimizer.pkl\")\n",
    "        with open(optimizer_path, 'wb') as f:\n",
    "            pickle.dump(weight_values, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, save_folder=\".\", learning_rate=0.0001):\n",
    "        # Load the parameters:\n",
    "        parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
    "        with open(parameters_path, \"rb\") as f:\n",
    "            parameters = pickle.load(f)\n",
    "        loaded_autoencoder = VAE(*parameters)\n",
    "\n",
    "        # Compile the model (to be able to retrain it with additional data if needed)\n",
    "        loaded_autoencoder.compile_model(learning_rate=learning_rate)\n",
    "\n",
    "        # Load the Weights\n",
    "        weights_path = os.path.join(save_folder, \"weights.h5\")\n",
    "        loaded_autoencoder.model.load_weights(weights_path)\n",
    "\n",
    "        # Load the Optimizer State\n",
    "        loaded_autoencoder.model._make_train_function()\n",
    "\n",
    "        optimizer_path = os.path.join(save_folder, \"optimizer.pkl\")\n",
    "        with open(optimizer_path, 'rb') as f:\n",
    "            weight_values = pickle.load(f)\n",
    "        loaded_autoencoder.model.optimizer.set_weights(weight_values)\n",
    "\n",
    "        return loaded_autoencoder\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "-------------\n",
    "Custom Layer\n",
    "-------------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LambdaLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, latent_space_dim):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mu, log_variance = inputs\n",
    "        epsilon = K.random_normal(shape=K.shape(self.latent_space_dim), mean=0., stddev=1.)\n",
    "        sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
    "        return sampled_point\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'latent_space_dim': self.latent_space_dim}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca553f-0f8e-4e5a-8ff2-60da87a30f57",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "First we need to load the snippets we created. If you want to test this code with your own data, please change the \"load_path\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "872cf87c-fe60-4dfb-a6cb-2f6d88bd7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder = \"4.0_256\"\n",
    "load_path = os.path.join(\"data_and_models\", subfolder)\n",
    "load_path = os.path.join(load_path, \"spectos500.npy\")\n",
    "data = np.load(load_path)\n",
    "x_train, x_test, _, _ = train_test_split(data, data, test_size=0.2)\n",
    "del data\n",
    "history = None\n",
    "val_history=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775b220-6970-4aae-ad36-31b21ce844b2",
   "metadata": {},
   "source": [
    "## Building the VAE\n",
    "Now we can build our model. Both the encoder and the decoder consist of five convolutional layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6461c773-c397-47aa-a452-7722e56aaec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dearvr-Tester/Documents/MetaHuman/venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"Valerio\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 256, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         (None, 128)               2100832   \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 256, 1)       665185    \n",
      "=================================================================\n",
      "Total params: 2,766,017\n",
      "Trainable params: 2,763,073\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_1 (Conv2D)   (None, 64, 128, 512) 5120        encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_1 (ReLU)           (None, 64, 128, 512) 0           encoder_conv_layer_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_1 (BatchNormalizatio (None, 64, 128, 512) 2048        encoder_relu_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_2 (Conv2D)   (None, 32, 64, 256)  1179904     encoder_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_2 (ReLU)           (None, 32, 64, 256)  0           encoder_conv_layer_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_2 (BatchNormalizatio (None, 32, 64, 256)  1024        encoder_relu_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_3 (Conv2D)   (None, 16, 32, 128)  295040      encoder_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_3 (ReLU)           (None, 16, 32, 128)  0           encoder_conv_layer_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_3 (BatchNormalizatio (None, 16, 32, 128)  512         encoder_relu_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_4 (Conv2D)   (None, 8, 16, 64)    73792       encoder_bn_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_4 (ReLU)           (None, 8, 16, 64)    0           encoder_conv_layer_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_4 (BatchNormalizatio (None, 8, 16, 64)    256         encoder_relu_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_5 (Conv2D)   (None, 4, 16, 32)    18464       encoder_bn_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_5 (ReLU)           (None, 4, 16, 32)    0           encoder_conv_layer_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_5 (BatchNormalizatio (None, 4, 16, 32)    128         encoder_relu_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_flatten (Flatten)       (None, 2048)         0           encoder_bn_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "min_vector_mu (Dense)           (None, 128)          262272      encoder_flatten[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "log_variance (Dense)            (None, 128)          262272      encoder_flatten[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_layer (LambdaLayer)      (None, 128)          0           min_vector_mu[0][0]              \n",
      "                                                                 log_variance[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,100,832\n",
      "Trainable params: 2,098,848\n",
      "Non-trainable params: 1,984\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "decoder_dense (Dense)        (None, 2048)              264192    \n",
      "_________________________________________________________________\n",
      "decoder_reshape_layer (Resha (None, 4, 16, 32)         0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 8, 16, 32)         9248      \n",
      "_________________________________________________________________\n",
      "decoder_relu_1 (ReLU)        (None, 8, 16, 32)         0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_1 (BatchNormaliza (None, 8, 16, 32)         128       \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 16, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "decoder_relu_2 (ReLU)        (None, 16, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_2 (BatchNormaliza (None, 16, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 32, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "decoder_relu_3 (ReLU)        (None, 32, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_3 (BatchNormaliza (None, 32, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 64, 128, 256)      295168    \n",
      "_________________________________________________________________\n",
      "decoder_relu_4 (ReLU)        (None, 64, 128, 256)      0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_4 (BatchNormaliza (None, 64, 128, 256)      1024      \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 128, 256, 1)       2305      \n",
      "_________________________________________________________________\n",
      "decoder_output_sigmoid (Acti (None, 128, 256, 1)       0         \n",
      "=================================================================\n",
      "Total params: 665,185\n",
      "Trainable params: 664,225\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Shape of the training data: (17399, 128, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "autoencoder = VAE(\n",
    "    input_shape=(x_train[0].shape[0], x_train[0].shape[1], x_train[0].shape[2]),\n",
    "    conv_filters=(512, 256, 128, 64, 32),\n",
    "    conv_kernels=(3, 3, 3, 3, 3),\n",
    "    conv_strides=(2, 2, 2, 2, (2, 1)),\n",
    "    latent_space_dim=128\n",
    ")\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "print(\"Shape of the training data: \" + str(x_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7294802-e79e-4bea-936e-780da361b1e6",
   "metadata": {},
   "source": [
    "## Train the VAE\n",
    "As we have limited memory on our machines we had to split the training process in several steps. Unfortunately the programme freezed on our machines from time to time. That's why we save the model after each step. With faster machines more epochs would have been possible, which could improve the results. \n",
    "\n",
    "Warning: This process can take a long time, depending on the loaded training data. If you want to train your own model, please change the \"save_path\" variable. If you only want to work with our trained models, don't execute the following code. Just jump to the next section \"Evaluate the training process\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28781b0f-3531-4579-8703-0d1c9ea692c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "\n",
    "autoencoder.compile_model(LEARNING_RATE)\n",
    "steps = 20\n",
    "history = []\n",
    "val_history = []\n",
    "\n",
    "for i in range(steps):\n",
    "    num = int(x_train.shape[0] / steps) * (i + 1)\n",
    "    test_num = int(x_test.shape[0] / steps) * (i + 1)\n",
    "\n",
    "    print(\"Start with subset \" + str(i+1) + \" of \" + str(steps))\n",
    "    print(\"Train from index \" + str(int(num - (num / (i + 1)))) + \" to index \" + str(num))\n",
    "    print(\"Use test indices \" + str(int(test_num - (test_num / (i + 1)))) + \" to \" + str(test_num) +\n",
    "          \" as validation set\")\n",
    "\n",
    "    train_subset = x_train[int(num - (num / (i + 1))):num]\n",
    "    test_subset = x_test[int(test_num - (test_num / (i + 1))):test_num]\n",
    "\n",
    "    step_history = autoencoder.train(train_subset, test_subset, BATCH_SIZE, EPOCHS)\n",
    "    history.extend(step_history.history['loss'])\n",
    "    val_history.extend(step_history.history['val_loss'])\n",
    "    \n",
    "    \"\"\"\n",
    "    ----------------\n",
    "    Save VAE\n",
    "    ----------------\n",
    "    \"\"\"\n",
    "\n",
    "    save_path = os.path.join(\"data_and_models\", subfolder)\n",
    "    name = \"VAE_Vocals_\" + str(autoencoder.latent_space_dim) + \"D_\" + \\\n",
    "           str(autoencoder.num_of_train_data) + \"samples_\" + str(EPOCHS) + \"Epochs\"\n",
    "    model_path = os.path.join(save_path, name)\n",
    "    autoencoder.save(model_path)\n",
    "\n",
    "    print(\"saved at: \" + save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71cebf-2107-4632-a818-6a87f66aaf82",
   "metadata": {},
   "source": [
    "## Evaluate the training process\n",
    "We stored the loss of each epoch of each step of the training and can look at it. This graph shows how the loss gets less with every epoch. The loss is defined as the mean-squared error between a given mel-spectogram and the mel-spectogram reproduced by the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e42c5127-da74-40ed-a157-4073cbd1f037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_path = os.path.join(\"data_and_models\", subfolder)\n",
    "\n",
    "if history is not None:\n",
    "    \"\"\"\n",
    "    Save the History\n",
    "    \"\"\"\n",
    "    history = np.asarray(history)\n",
    "    val_history = np.asarray(val_history)\n",
    "\n",
    "    hist_save_path = os.path.join(save_path, \"history.npy\")\n",
    "    val_hist_save_path = os.path.join(save_path, \"val_history.npy\")\n",
    "\n",
    "\n",
    "\n",
    "    with open(hist_save_path, 'wb') as f:\n",
    "        np.save(f, history)\n",
    "        np.save(f, val_history)\n",
    "else:\n",
    "    hist_save_path = os.path.join(save_path, \"history.npy\")\n",
    "    with open(hist_save_path, 'rb') as f:\n",
    "         history = np.load(f)\n",
    "         val_history = np.load(f)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.title(\"Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(val_history)\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.xlabel(\"Validation Loss\")\n",
    "# plt.title(\"Validation Loss History\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
