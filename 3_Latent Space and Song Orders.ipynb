{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cc27aa-98ee-406a-b078-411bda29ada0",
   "metadata": {},
   "source": [
    "## Latent Space and Song Orders\n",
    "We can now look at the trained samples in latent space. The latent space is a 2 dimensional representation of the wavesets ordered by similiarity. Each waveset is a point in the 2D space. We can think of a song as a series of jumps from waveset to waveset, i.e. a series of 2D coordinates. We want to save these series and train another network to create a new series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc662cf-19b9-4f6e-a57f-d5ccca1325ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Variational_Autoencoder_alla_Valerio import VAE as Autoencoder\n",
    "from Snippets import Snippets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import librosa.feature.inverse\n",
    "from IPython.display import display, Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2531c7-992e-47b9-a952-abde97bc3d61",
   "metadata": {},
   "source": [
    "First we load the trained autoencoder and the training data from our disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d3b8fc8-3f7a-400d-93e5-714c841d1a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Daten\\Studium\\Semester_7\\MusikInfo\\MetaHuman\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"Valerio\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 16, 1)]      0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         (None, 128)               1609312   \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 16, 1)        417505    \n",
      "=================================================================\n",
      "Total params: 2,026,817\n",
      "Trainable params: 2,023,873\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 16, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_1 (Conv2D)   (None, 64, 8, 512)   5120        encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_1 (ReLU)           (None, 64, 8, 512)   0           encoder_conv_layer_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_1 (BatchNormalizatio (None, 64, 8, 512)   2048        encoder_relu_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_2 (Conv2D)   (None, 32, 4, 256)   1179904     encoder_bn_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_2 (ReLU)           (None, 32, 4, 256)   0           encoder_conv_layer_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_2 (BatchNormalizatio (None, 32, 4, 256)   1024        encoder_relu_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_3 (Conv2D)   (None, 16, 2, 128)   295040      encoder_bn_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_3 (ReLU)           (None, 16, 2, 128)   0           encoder_conv_layer_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_3 (BatchNormalizatio (None, 16, 2, 128)   512         encoder_relu_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_4 (Conv2D)   (None, 8, 1, 64)     73792       encoder_bn_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_4 (ReLU)           (None, 8, 1, 64)     0           encoder_conv_layer_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_4 (BatchNormalizatio (None, 8, 1, 64)     256         encoder_relu_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_layer_5 (Conv2D)   (None, 4, 1, 32)     18464       encoder_bn_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder_relu_5 (ReLU)           (None, 4, 1, 32)     0           encoder_conv_layer_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bn_5 (BatchNormalizatio (None, 4, 1, 32)     128         encoder_relu_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_flatten (Flatten)       (None, 128)          0           encoder_bn_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "min_vector_mu (Dense)           (None, 128)          16512       encoder_flatten[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "log_variance (Dense)            (None, 128)          16512       encoder_flatten[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_layer (LambdaLayer)      (None, 128)          0           min_vector_mu[0][0]              \n",
      "                                                                 log_variance[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,609,312\n",
      "Trainable params: 1,607,328\n",
      "Non-trainable params: 1,984\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "decoder_dense (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "decoder_reshape_layer (Resha (None, 4, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 8, 1, 32)          9248      \n",
      "_________________________________________________________________\n",
      "decoder_relu_1 (ReLU)        (None, 8, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_1 (BatchNormaliza (None, 8, 1, 32)          128       \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 16, 2, 64)         18496     \n",
      "_________________________________________________________________\n",
      "decoder_relu_2 (ReLU)        (None, 16, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_2 (BatchNormaliza (None, 16, 2, 64)         256       \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 32, 4, 128)        73856     \n",
      "_________________________________________________________________\n",
      "decoder_relu_3 (ReLU)        (None, 32, 4, 128)        0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_3 (BatchNormaliza (None, 32, 4, 128)        512       \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 64, 8, 256)        295168    \n",
      "_________________________________________________________________\n",
      "decoder_relu_4 (ReLU)        (None, 64, 8, 256)        0         \n",
      "_________________________________________________________________\n",
      "decoder_bn_4 (BatchNormaliza (None, 64, 8, 256)        1024      \n",
      "_________________________________________________________________\n",
      "decoder_conv_transpose_layer (None, 128, 16, 1)        2305      \n",
      "_________________________________________________________________\n",
      "decoder_output_sigmoid (Acti (None, 128, 16, 1)        0         \n",
      "=================================================================\n",
      "Total params: 417,505\n",
      "Trainable params: 416,545\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "subfolder = \"0.25_16\"\n",
    "model_name = \"Valerio_128D_300samples_20Epochs\"\n",
    "autoencoder = Autoencoder.load(\"data_and_models\\\\\" + subfolder +\"\\\\\" + model_name)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916aa838-6cda-4663-8e33-aef1fcbce416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(subfolder):\n",
    "    spectogram_data = np.load(\"data_and_models\\\\\" + subfolder + \"\\\\spectos500.npy\")\n",
    "    song_labels = np.load(\"data_and_models\\\\\" + subfolder + \"\\\\song_labels500.npy\")\n",
    "    position_labels = np.load(\"data_and_models\\\\\" + subfolder + \"\\\\position_labels500.npy\")\n",
    "    print(spectogram_data.shape)\n",
    "    \n",
    "    return spectogram_data, position_labels, song_labels\n",
    "\n",
    "x_train, y_train, y_train_alt = load_data(subfolder)\n",
    "\n",
    "x_train = x_train[0:500]\n",
    "y_train = y_train[0:500]\n",
    "y_train_alt = y_train_alt[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9cde54-0ccf-40b3-b100-acdf873d6592",
   "metadata": {},
   "source": [
    "## Data in latent space\n",
    "We can now plot our wavesets in the latent space. Each waveset is reduced to a point in the 2 dimensional space. The distance between the points of two wavesets corresponds to their similarity. The colors represent at which position in a song the waveset occured (first 10% of the song, second 10% of the song and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c36c0-eb49-4525-b601-952050114f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_representation = autoencoder.encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd247a-1587-46a4-9ef5-3a156cc16093",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(latent_representation[:, 0], latent_representation[:, 1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1ba30-e811-42f1-a45f-06ab53f33082",
   "metadata": {},
   "source": [
    "## Generate Song Orders\n",
    "A song is a series of wavesets. This series of wavesets corresponds to a list of coordinates in the latent space. It can be imagined as a movement - or more precisely a sequence of jumps - through the latent space. Later we want to use these sequences as training data for a second network. On their basis the second network will be able to produce new sequences. For this we need to store the series of coordinates for each song. We create a list of 2D arrays: first dimension is the song, and second dimension is the a tuple of coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313edc50-06b9-4b60-8a5d-235fcd380c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_snippets_per_song = np.load(\"data_and_models\\\\\" + subfolder + \"\\\\SnippetNum500.npy\")\n",
    "\n",
    "ws_sums = np.cumsum(num_of_snippets_per_song)\n",
    "num_of_snippets_per_song = num_of_snippets_per_song[:np.argwhere(ws_sums > x_train.shape[0]).min()]\n",
    "\n",
    "start_ws = 0\n",
    "stop_ws = 0\n",
    "song_orders = []\n",
    "\n",
    "for i, num_of_snippet in enumerate(num_of_snippets_per_song):\n",
    "    stop_ws += num_of_snippet\n",
    "    song_order = latent_representation[start_ws : stop_ws]\n",
    "    song_order = np.reshape(song_order, (num_of_snippet, autoencoder.latent_space_dim))\n",
    "    song_orders.append(song_order)\n",
    "    start_ws += num_of_snippet\n",
    "    \n",
    "song_orders = np.asarray(song_orders, dtype=object)\n",
    "song_orders.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1cd59-1f89-4371-9448-272ea2fdc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_orders = latent_representation[:500].reshape(1,500,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98f8d6-37d5-4be6-9270-abfe1a1d0d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a519461b-c668-4dff-8790-a9c9be1442c6",
   "metadata": {},
   "source": [
    "We save the new list of song_orders on disk for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f399e-f00a-4642-8897-9ae334b8fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"data_and_models\\\\\" + subfolder + \"\\\\\" + str(autoencoder.model.name) +\"_\" + str(autoencoder.num_of_train_data)+\"song_orders500\" + \".npy\"\n",
    "np.save(save_path,song_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a67c5f9-0a77-42cc-8f1b-1a370f16ef80",
   "metadata": {},
   "source": [
    "## The latent space representation of one song\n",
    "We can plot the latent representation of a song. The subsequent wavesets are connected by lines to visualize, that the sequence of wavesets corresponds to a path trough the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37005c8-33b8-4f2a-bf14-a48063cac5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_num = 0#np.random.randint(0, num_of_snippets_per_song.shape[0])\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.plot(song_orders[song_num][:, 0], song_orders[song_num][:, 1], '-.o', markersize=5, markerfacecolor='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e4d92-8d1f-41a7-af54-f42aea697f62",
   "metadata": {},
   "source": [
    "## Create a new Waveset\n",
    "We can try the conversion, by letting the autoencoder reconstruct a junk of audio. We feed the spectogram of the audio to the encoder to get it's latent representation. We can then test how much information gets lost in the autoencoder by passing the latent_representation to our decoder, which will spit out a reconstructed spectogram, which we can then use to reconstruct pca-data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478aab1d-60b7-4777-8029-ed9a2bb23c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_LENGTH = 690*2\n",
    "HOP_LENGTH = 690\n",
    "N_FFT = 690*2\n",
    "\n",
    "index = np.random.randint(0, x_train.shape[0]) # Choose a random snippet as the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883cebe-9e8b-446c-a09f-65977dd8789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_signal, original_spectos = Snippets.reconstructed_spectos_to_pcm(x_train[index:index+20], hop_length=HOP_LENGTH, n_fft=N_FFT, win_length=WIN_LENGTH)\n",
    "Snippets.plot_specto(original_spectos, \"Original Specto\", HOP_LENGTH)\n",
    "print(\"\\n This is the original Audio:\")\n",
    "display(Audio(original_signal,rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cce01-8884-4856-ad38-8fcc5cd1fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_signal, recon_specto = Snippets.specto_to_pcm(data=x_train[index:index+20], \n",
    "                                                    model=autoencoder, \n",
    "                                                    hop_length=HOP_LENGTH, \n",
    "                                                    n_fft=N_FFT, \n",
    "                                                    win_length=WIN_LENGTH)\n",
    "Snippets.plot_specto(recon_specto, \"Reconstructed Specto\", HOP_LENGTH)\n",
    "print(\"\\n This is the reconstructed Audio:\")\n",
    "display(Audio(recon_signal,rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03f9e0-0033-45b1-bbe7-5915b86ecfe8",
   "metadata": {},
   "source": [
    "We can also try what happens, if we use random points in the latent space and use them as coordinates for our song. We sample a random point, reconstruct the corresponding spectogram and use it to reconstruct pca-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf84e30-7de9-48e4-a680-4d50b0109e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_points = np.random.rand(40, latent_representation.shape[1])\n",
    "random_points = (random_points * 2) -1\n",
    "random_signal, random_spectos = Snippets.latent_representation_to_pca(latent_representation=random_points,\n",
    "                                                                      model=autoencoder,\n",
    "                                                                      hop_length=HOP_LENGTH, \n",
    "                                                                      n_fft=N_FFT, \n",
    "                                                                      win_length=WIN_LENGTH)\n",
    "Snippets.plot_specto(random_spectos, \"Random Specto\", HOP_LENGTH)\n",
    "print(\"\\n This is the reconstruction of random points in the latent space:\")\n",
    "display(Audio(random_signal,rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f32d8-4ded-4d0c-a601-b00821dfde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40cc4c4-3c3a-42c5-bd6c-85aeb6a1d00d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
