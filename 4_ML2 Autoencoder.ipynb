{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db49fd81-bdab-4796-88aa-d7fecab98d4e",
   "metadata": {},
   "source": [
    "## Autoencoder for Song Orders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0f6625-0b39-4094-a25b-e6cc17f39654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Autoencoder_alla_Valerio import Autoencoder\n",
    "from Variational_Autoencoder_alla_Valerio import VAE\n",
    "from LSTM_Autoencoder import LSTM_Autoencoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Snippets import Snippets\n",
    "from IPython.display import display, Audio\n",
    "import librosa\n",
    "from tensorflow.keras.layers import Masking\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53472cc-5854-46d4-abb7-6e3fcfefccd8",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "We load the array with all song_orders. We need to bring them all to the same size, so we can use them as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015980d5-860c-4353-a4fc-36881bb4529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder = \"0.25_16\"\n",
    "model_name = \"Valerio_23927\"\n",
    "song_orders = np.load(\"data_and_models\\\\\" + subfolder + \"\\\\\" + model_name +\"song_orders.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4568a446-6e28-4489-827f-bc820d1a7eb0",
   "metadata": {},
   "source": [
    "We can normalise the data and denormalise it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abf86b7-596e-4b30-b041-92811d9b8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_orders = np.asarray([item for order in song_orders for item in order])\n",
    "# old_min = flattened_orders.min()\n",
    "# old_max = flattened_orders.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2de087-250a-439c-bd34-58d3a30b8094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d492308a-0353-44e8-8130-b4c2f6bc6263",
   "metadata": {},
   "source": [
    "For the training of the autoencoder, we need all song-orders to be of the same length. Additionally the length has to be an even number. We bring each song to the length of the longest song by adding the value -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd97a845-a366-47ec-a7d4-2b30a0c62254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest song had: 2120snippets\n",
      "Our trainingsdata has the shape: (22, 2120, 128)\n"
     ]
    }
   ],
   "source": [
    "max_song_length = 0\n",
    "\n",
    "for i in range(0, len(song_orders)):\n",
    "    if len(song_orders[i]) > max_song_length:\n",
    "        max_song_length = len(song_orders[i])\n",
    "        if not (max_song_length % 2) == 0: #increase max_song_length to even number. This is crutial for the Shape of Autoencoder input data.\n",
    "            max_song_length += 1       \n",
    "print(\"The longest song had: \" + str(max_song_length) +\"snippets\")\n",
    "\n",
    "x_train = []\n",
    "for order in song_orders:\n",
    "    if order.shape[0] < max_song_length:\n",
    "        padded_order = np.pad(order,((0,max_song_length-order.shape[0]),(0,0)), constant_values=(0,0))\n",
    "        x_train.append(padded_order)\n",
    "x_train = np.asarray(x_train)\n",
    "#x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2], 1)\n",
    "\n",
    "print(\"Our trainingsdata has the shape: \" + str(x_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562eea6b-aed1-473e-947b-255933909082",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_min = x_train.min()\n",
    "old_max = x_train.max()\n",
    "\n",
    "def normalise(array, new_min, new_max, old_min, old_max):\n",
    "        norm_array = (array - old_min) / (old_max - old_min)\n",
    "        norm_array = norm_array * (new_max - new_min) + new_min\n",
    "        return norm_array\n",
    "\n",
    "x_train = normalise(x_train, 0, 1, old_min, old_max)\n",
    "mask_value=x_train[0][-1][-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49bd12-5f8a-4d35-8bf0-fd6b678efb08",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "We can build a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c30423-a460-4842-a126-bd2b5bedffa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder = LSTM_Autoencoder(\n",
    "    input_shape=(x_train.shape[1],x_train.shape[2]),\n",
    "    lstm_dims=[],\n",
    "    latent_space_dim=32,\n",
    "    mask_value=mask_value\n",
    ")\n",
    "#autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841bb511-0ed3-4c3e-93d9-c6840fc8943c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22 samples\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - 5s 238ms/sample - loss: nan\n",
      "Epoch 2/100\n",
      "15/22 [===================>..........] - ETA: 1s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      5\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile_model(LEARNING_RATE)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Daten\\Studium\\Semester_7\\MusikInfo\\MetaHuman\\LSTM_Autoencoder.py:80\u001b[0m, in \u001b[0;36mLSTM_Autoencoder.train\u001b[1;34m(self, x_train, batch_size, num_epochs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_train, batch_size, num_epochs):\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     85\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Daten\\Studium\\Semester_7\\MusikInfo\\MetaHuman\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:795\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    794\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Daten\\Studium\\Semester_7\\MusikInfo\\MetaHuman\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py:644\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    641\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    642\u001b[0m   val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Daten\\Studium\\Semester_7\\MusikInfo\\MetaHuman\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py:380\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(mode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_index, batch_logs)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    382\u001b[0m   batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[1;32mD:\\Daten\\Studium\\Semester_7\\MusikInfo\\MetaHuman\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4054\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4049\u001b[0m     symbol_vals \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol_vals \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4050\u001b[0m     feed_symbols \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_symbols \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetches \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4051\u001b[0m     session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session):\n\u001b[0;32m   4052\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 4054\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4055\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4056\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches):])\n\u001b[0;32m   4057\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[0;32m   4059\u001b[0m     fetched[:\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[0;32m   4060\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Daten\\Studium\\Semester_7\\MusikInfo\\MetaHuman\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1480\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1479\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1480\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1483\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1484\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE =  0.0001\n",
    "BATCH_SIZE = 3\n",
    "EPOCHS = 100\n",
    "\n",
    "autoencoder.compile_model(LEARNING_RATE)\n",
    "autoencoder.train(x_train, BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfa91c-2e31-497e-b347-6ed2b5a8c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(\"Autoencoder_SongOrders\" + str(autoencoder.latent_space_dim) + \"D_\" + subfolder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28e8ef-dbd7-41b1-945d-4233586feadc",
   "metadata": {},
   "source": [
    "# Check Results\n",
    "Check how good the autoencoder is in reconstructing a song order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb820d0-36bb-49bb-ae1d-10590621f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_representation = autoencoder.encoder.predict(x_train)\n",
    "reconstructed_data = autoencoder.decoder.predict(latent_representation)\n",
    "reconstructed_data = Snippets._denormalise(reconstructed_data, 0, 1, old_min, old_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b704b-d9d2-410d-900d-65eb50b27b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Snippets._denormalise(x_train, 0, 1, old_min, old_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83554a02-0325-4704-890a-e50e044b8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_num = 0\n",
    "\n",
    "print(\"This is the original\")\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.plot(x_train[song_num][:, 0], x_train[song_num][:, 1], '-.o', markersize=5, markerfacecolor='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8b38c-6a57-4ab6-b011-e4652ad1c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the reconstruction\")\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.plot(reconstructed_data[song_num][:, 0], reconstructed_data[song_num][:, 1], '-.o', markersize=5, markerfacecolor='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb93e1-b55b-46ce-b20c-4ecfe918d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_LENGTH = 690*2\n",
    "HOP_LENGTH = 690\n",
    "N_FFT = 690*2\n",
    "\n",
    "snippet_model_name = \"VAE_Vocals_128D_23927samples_20Epochs\"\n",
    "snippet_autoencoder = VAE.load(\"data_and_models\\\\\" + subfolder +\"\\\\\" + snippet_model_name)\n",
    "\n",
    "reconstructed_data = reconstructed_data.reshape(reconstructed_data.shape[0], reconstructed_data.shape[1], reconstructed_data.shape[2])\n",
    "reconstructed_order = reconstructed_data[song_num]\n",
    "print(reconstructed_order.shape)\n",
    "      \n",
    "reconstructed_signal, reconstructed_spectos = Snippets.latent_representation_to_pca(latent_representations=reconstructed_order,\n",
    "                                                                                    model=snippet_autoencoder, \n",
    "                                                                                    hop_length=HOP_LENGTH, \n",
    "                                                                                    n_fft=N_FFT, \n",
    "                                                                                    win_length=WIN_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f5db2-15c0-434d-b7ac-cfe7ef33636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(reconstructed_signal, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d92c24-91cd-40ac-ad97-472cf4c748b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'demo_data\\\\stems'\n",
    "paths = librosa.util.find_files(folder_path, ext=['wav'])\n",
    "original_song, _ = librosa.load(paths[song_num],sr=44100, mono=True)\n",
    "\n",
    "print(\"This is the original song: \\n\")\n",
    "display(Audio(original_song, rate=44100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb025fe-b77a-4734-9b87-fd2a5e7822bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f2091b4-4d68-4bbb-b22a-72ffa841fbc3",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/lstm-autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8512242-b7ab-4399-b5e8-18d74fe9589c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
